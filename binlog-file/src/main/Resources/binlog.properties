#hdfs
#hdfs.temp.url hdfs://dn0:8020/data/temp
hdfs.temp.url=hdfs://localhost:9000/data/temp
#hdfs.warehouse.url hdfs://dn0:8020/data/warehouse
hdfs.warehouse.url=hdfs://localhost:9000/data/warehouse
#jdbc properties
jdbc.driverClassName=com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://cloudera1:3306/default?useUnicode=true&characterEncoding=utf8
jdbc.username=cloudera
jdbc.password=cloudera0701!
#jdbc.url=jdbc:mysql://cloudera3:3306/default?useUnicode=true&characterEncoding=utf8
#jdbc.username=root
#jdbc.password=dashu0701
jdbc.database=binlog
TiDB properties
tidb.driverClassName=com.mysql.jdbc.Driver
tidb.url=jdbc:mysql://TiDB1:3306/default?useUnicode=true&characterEncoding=utf8
tidb.username=tidb_rd
tidb.password=T87MQVWJKUo8VXeV
#task
queue.mode=default
max.thread.binlog.thread=3
#runner.class=com.datatrees.datacenter.resolver.TaskProcessor
#processCheck 任务超时时间
process.check.interval=10
#processCheck 任务超时时间刻度 hour\minute\second
process.check.time.scale=minute
#processCheck 定时任务启动延时
process.check.schedule.task.initaildelay=0
#processCheck 定时任务定时执行间隔(时间刻度为分钟)
process.check.schedule.task.period=10
#retry 次数
process.check.schedule.task.retry=3
runner.class=com.datatrees.datacenter.transfer.process.TransferTimerTask
#binlog下载多线程线程池相关设置
thread.pool.corePoolSize=5
thread.pool.maximumPoolSize=15
thread.pool.keepAliveTime=3
#binlog下载定时任务时间参数
#AliBinLogFileTransfer 定时任务启动延时
AliBinLogFileTransfer.check.schedule.task.initaildelay=0
#AliBinLogFileTransfer 定时任务定时执行间隔(时间刻度为分钟)
AliBinLogFileTransfer.check.schedule.task.period=2
AliBinLogFileTransfer.schedule.task.exclude.start=0
AliBinLogFileTransfer.schedule.task.exclude.end=6
#schema
port=3306
user=debezium
password=Debezium_
#aliyun properties
# aliyun 相关配置
REGION_ID=cn-hangzhou
ACCESS_KEY_ID=LTAIAfBoz0Wz5O6L
ACCESS_SECRET=WGlWEscL3u5rfFrokhYle4jFXsXBv9
#截取文件正则
REGEX_PATTERN=(mysql-bin\\.)(.*)(\\.tar)|(mysql-bin\\.)(\\d+)
#hdfs文件路径
HDFS_PATH=hdfs://cloudera3:8020/pc
#HDFS_PATH=hdfs://master1:8020/pc
BINLOG_ACTION_NAME=DescribeBinlogFiles
PAGE_SIZE=30
DOWN_TIME_INTERVAL=800
DBINSTANCE_LIST=rm-bp13sd5q1x8416p3p,rm-bp1py1768bl28z5zs,rm-bp1x3mt0ogici00l5,rm-bp1bgyn06wua3s262,rm-bp1tcbap8lvphh08x,rm-bp1v9i4pwb73467g5,rm-bp15112z7tv876mzl,rm-bp101325tnut8me53
BUFFER_SIZE=1024
#redis
redis.server=master0:6379
task.queue=taskQueue
#partition
update=LastUpdateDate,lastupdate_date,UpdateDate,last_update_date,last_update_time,LastUpdate,UpdateTime,update_date,updatedAt,LastUpdatedDatetime,LastUpdateTime
create=create_time,CreateDate,create_date,CreateTime,CreatTime,create_date,createtime,createdate,createTime,createdAt,CreatedDatetime
id=id,Id,iD,ID
#binlog-service
subscribe.topics=dbhistory.*
#binlog-service kafka consumer properties
binlog.service.kafka.bootstrap.servers=kafka1:9092
binlog.service.kafka.group.id=client-history-loader-test-1
binlog.service.kafka.enable.auto.commit=false
binlog.service.kafka.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
binlog.service.kafka.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
binlog.service.kafka.auto.offset.reset=earliest
binlog.service.kafka.max.partition.fetch.bytes=5242880
AVRO_HDFS_PATH=hdfs://cloudera3/data/warehouse/update
HIVE_HDFS_PATH=hdfs://cloudera3/user/hive/warehouse2/collection.db
SAMPLE_FACTOR=400
FILE_NUM=0
RECORD_NUM=1000

#IDC 主机信息
SERVER_IP=10.1.2.197,10.1.2.198,10.1.2.199,10.1.3.114
USER_NAME=root
PASSWORD=
PORT=22
PRIVATE_KEY=/Users/personalc/.ssh/id_rsa
SERVER_ROOT=/data1/application/binlog-process/log/
CLIENT_ROOT=/Users/personalc/test/