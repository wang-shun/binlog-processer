#hdfs
#hdfs.temp.url hdfs://dn0:8020/data/temp
#hdfs.temp.url hdfs://cloudera2:8020/data/temp
temp.url /Users/zxding1986/Downloads/data_temp
#temp.url /data0/application/binlog-process/tmp
#hdfs.warehouse.url hdfs://dn0:8020/data/warehouse
#hdfs.warehouse.url hdfs://cloudera2:8020/data/warehouse
#hdfs.warehouse.url hdfs://nameservice1/data/warehouse
hdfs.warehouse.url hdfs://localhost:9000/data/warehouse
#hdfs.site /Users/zxding1986/src/ideaworkbench/CDH/exe/hadoop-2.6.0-cdh5.12.1/etc/hadoop/core-site.xml
jdbc.database=binlog
#task
queue.dispense.class=com.datatrees.datacenter.core.task.RabbitMqDispensor
queue.process.class=com.datatrees.datacenter.resolver.RabbitMqProcessor
queue.server=hadoop13
queue.port=5672
queue.topic=taskQueue_local
queue.compare.topic=task.compare.queue
#queue.topic=taskQueue
max.thread.binlog.thread=10
#写入avro的buffer大小
avro.message.buffer.size=1000
#runner.class=com.datatrees.datacenter.resolver.RabbitMqProcessor
runner.class=com.datatrees.datacenter.transfer.process.TransferTimerTask
#runner.class=com.datatrees.datacenter.CompareTask
#processCheck 任务超时时间
process.check.interval=10
#processCheck 任务超时时间刻度 hour\minute\second
process.check.time.scale=minute
#processCheck 定时任务启动延时
process.check.schedule.task.initaildelay=0
#processCheck 定时任务定时执行间隔(时间刻度为分钟)
process.check.schedule.task.period=10
#retry 次数
process.check.schedule.task.retry=3
#binlog下载多线程线程池相关设置
thread.pool.corePoolSize=5
thread.pool.maximumPoolSize=15
thread.pool.keepAliveTime=3
#binlog下载定时任务时间参数
#AliBinLogFileTransfer 定时任务启动延时
AliBinLogFileTransfer.check.schedule.task.initaildelay=0
#AliBinLogFileTransfer 定时任务定时执行间隔(时间刻度为分钟)
AliBinLogFileTransfer.check.schedule.task.period=30
#不下载事件段开始时刻
#AliBinLogFileTransfer.schedule.task.exclude.start=
#不下载事件段结束时刻
#AliBinLogFileTransfer.schedule.task.exclude.end=
#schema
#port 3307
port 3306
#user root
user debezium
#password 123456
password Debezium_
# aliyun 相关配置
REGION_ID=cn-hangzhou
ACCESS_KEY_ID=LTAIAfBoz0Wz5O6L
ACCESS_SECRET=WGlWEscL3u5rfFrokhYle4jFXsXBv9
#每页请求的文件数量
PAGE_SIZE=30
#下载事件间隔
DOWN_TIME_INTERVAL=1500
#截取文件正则
REGEX_PATTERN=(mysql-bin\\.)(\\d+)
#hdfs文件路径
HDFS_PATH=hdfs://cloudera3/pc
DBINSTANCE_LIST=10.1.2.197,10.1.2.198,10.1.2.199,10.1.3.114
#DBINSTANCE_LIST=rm-bp1h5j9w2o9335zsn,rm-bp1cowwkt73ni6271,rm-bp1p4s8pgekg2di55,rm-bp1dq0cr4kk7j85ff,rm-bp12z0brmm2d799lm,rm-bp1hzldabba9nhm68,rm-bp19wpb29u91pbf6p,rm-bp159o69998x3xe34,rm-bp155hg16d7t501f6,rm-bp1382ojqe4u01d1t,rm-bp1d1ac1fa41jzf28,rm-bp11gox03jgt2ullb,rm-bp1xvc22l77p850ja,rm-bp14hbsc7ji8ivy7r,rm-bp1zaf4i3hnci3p6j,rm-bp1l1v7b7b2d2p8k3,rdss453q339d02wzev02,rdspp6t6sg70l3aw79gs,rds69zmdrvc65851511d
#DBINSTANTCE_2
#DBINSTANCE_LIST=rm-bp13sd5q1x8416p3p,rm-bp1py1768bl28z5zs,rm-bp1x3mt0ogici00l5,rm-bp1bgyn06wua3s262,rm-bp1tcbap8lvphh08x,rm-bp1v9i4pwb73467g5,rm-bp15112z7tv876mzl,rm-bp101325tnut8me53,rm-bp17mz8twp1c71zk2,rm-bp18uypc15s63z5xa,rm-bp19ok2k1h1dcwk6e,rm-bp14007m55u6a9173,rm-bp114u46sxgr719pf,rm-bp1htjis5cxoufia6,rm-bp1k0lx0x43542h47,rm-bp11fjyus6cugmvd2,rdsx466714015460kwu2,rdsuu2y47xbx0gv4nxp8,rdsf886ti672b2917z1u
#DBINSTANCE_LIST=rm-bp1h5j9w2o9335zsn,rm-bp1cowwkt73ni6271,rm-bp1p4s8pgekg2di55,rm-bp1dq0cr4kk7j85ff,rm-bp12z0brmm2d799lm,rm-bp1hzldabba9nhm68,rm-bp19wpb29u91pbf6p,rm-bp159o69998x3xe34,rm-bp155hg16d7t501f6,rm-bp1382ojqe4u01d1t,rm-bp1d1ac1fa41jzf28,rm-bp11gox03jgt2ullb,rm-bp1xvc22l77p850ja,rm-bp14hbsc7ji8ivy7r,rm-bp1zaf4i3hnci3p6j,rm-bp1l1v7b7b2d2p8k3,rdss453q339d02wzev02,rdspp6t6sg70l3aw79gs,rds69zmdrvc65851511d,rm-bp13sd5q1x8416p3p,rm-bp1py1768bl28z5zs,rm-bp1x3mt0ogici00l5,rm-bp1bgyn06wua3s262,rm-bp1tcbap8lvphh08x,rm-bp1v9i4pwb73467g5,rm-bp15112z7tv876mzl,rm-bp101325tnut8me53,rm-bp17mz8twp1c71zk2,rm-bp18uypc15s63z5xa,rm-bp19ok2k1h1dcwk6e,rm-bp14007m55u6a9173,rm-bp114u46sxgr719pf,rm-bp1htjis5cxoufia6,rm-bp1k0lx0x43542h47,rm-bp11fjyus6cugmvd2,rdsx466714015460kwu2,rdsuu2y47xbx0gv4nxp8,rdsf886ti672b2917z1u
#redis
redis.master.name=mymaster
redis.sentinel.address=redis://10.1.2.210:36379,redis://10.1.2.209:36379
#partition
id=id,Id,iD,ID
update=LastUpdateDate,lastupdate_date,UpdateDate,last_update_date,last_update_time,LastUpdate,UpdateTime,update_date,updatedAt,LastUpdatedDatetime,LastUpdateTime,UPDATED_DATETIME,UpdatedDatetime,UpdateAt,lastupdatedate,lastUpdateDate,last_modified_date,modify_date,updatetime
create=CREATED_DATETIME,create_time,CreateDate,create_date,CreateTime,CreatTime,create_date,createtime,createdate,createTime,createdAt,CreatedDatetime,create_date_time,CreateAt,creation_date
#binlog-service
subscribe.topics=dbhistory.*
#binlog-service kafka consumer properties
binlog.service.kafka.bootstrap.servers=kafka1:9092
binlog.service.kafka.group.id=client-history-loader-test-1
binlog.service.kafka.enable.auto.commit=false
binlog.service.kafka.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
binlog.service.k
afka.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
binlog.service.kafka.auto.offset.reset=earliest
binlog.service.kafka.max.partition.fetch.bytes=5242880
#compare
#binlog 解析出的Avro存放根目录
AVRO_HDFS_PATH=hdfs://cloudera2/data/warehouse_idc
#hive orc 文件根目录
HIVE_HDFS_PATH=hdfs://cloudera3
#是否对检查的数据进行抽样（yes/no）
SAMPLE_FLAG=no
#一下两个参数用于调整比较调度策略
#文件数量
FILE_NUM=0
#记录数量
RECORD_NUM=0
#IDC 主机信息(多个主机IP逗号分隔)
SERVER_IP=10.1.2.197,10.1.2.198,10.1.2.199,10.1.3.114
USER_NAME=root
PASSWORD=
PORT=22
PRIVATE_KEY=/Users/personalc/.ssh/id_rsa
SERVER_ROOT=/data1/application/binlog-process/log
CLIENT_ROOT=/Users/personalc/test/
USE_PASSWORD=false
#远程服务器类型（aliyun/idc）
SERVER_TYPE=idc

#数据检查分区类型
partition.type=create
ID_LIST_MAX=50000